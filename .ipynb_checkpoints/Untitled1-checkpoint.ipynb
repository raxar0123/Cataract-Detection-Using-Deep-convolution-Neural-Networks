{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae0e87a-8ec9-49f2-9bee-08f2bb031cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing dependencies\n",
    "import os\n",
    "from pathlib import Path \n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "102f2719-2d1b-4b27-b887-fcabb2c5e2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Resizing all the images into a fixed size (256x256 pixel)\n",
    "p = Path(\"dataset/\")\n",
    "dirs = p.glob(\"*\")\n",
    "\n",
    "image_dataset = []\n",
    "labels = []\n",
    "labelsToImg = {0: \"Normal\", 1: \"Cataract\"}\n",
    "labelDict = {\"Normal\": 0, \"Cataract\": 1}\n",
    "\n",
    "for folder_dir in dirs:\n",
    "    label = str(r\"C:\\Users\\RAHUL\\Documents\\Cataract-Prediction-main1\\Datasets\\ODIR-IMAGE\").split(\"\\\\\")[-1]\n",
    "\n",
    "    count = 0\n",
    "    for img_path in folder_dir.glob(\"*jpg\"):\n",
    "        img = image.load_img(img_path, target_size=(256, 256))  # resizing to 256x256 pixels\n",
    "        img_array = image.img_to_array(img)\n",
    "\n",
    "        image_dataset.append(img_array)\n",
    "        labels.append(labelDict.get(label))\n",
    "        count += 1\n",
    "    print(count)\n",
    "\n",
    "print(len(image_dataset))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d596a5-4141-4146-8af7-f8c6fc944257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (0,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Training and Validation split\u001b[39;00m\n\u001b[0;32m     12\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m X_Train \u001b[38;5;241m=\u001b[39m X[:split, :]\n\u001b[0;32m     15\u001b[0m Y_Train \u001b[38;5;241m=\u001b[39m Y[:split]\n\u001b[0;32m     17\u001b[0m X_Test \u001b[38;5;241m=\u001b[39m X[split:, :]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "X = np.array(image_dataset)\n",
    "Y = np.array(labels)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "X, Y = shuffle(X, Y, random_state=2)\n",
    "\n",
    "# Normalizing the data\n",
    "X = X / 255.0\n",
    "\n",
    "# Training and Validation split\n",
    "split = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_Train = X[:split, :]\n",
    "Y_Train = Y[:split]\n",
    "\n",
    "X_Test = X[split:, :]\n",
    "Y_Test = Y[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e37c1-88c2-4e89-8718-af855e5f8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a data generator for efficient memory usage\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(X_Train, Y_Train, batch_size=32)\n",
    "test_generator = test_datagen.flow(X_Test, Y_Test, batch_size=32)\n",
    "\n",
    "# Creating a sequential model\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcbf1b-870c-4cc9-860f-4616d22ad5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding layers\n",
    "model.add(Conv2D(filters=96, input_shape=(256, 256, 3), kernel_size=(11, 11), strides=(4, 4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f331a-7192-477e-8894-2e127ccaaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model using the data generator\n",
    "history = model.fit(train_generator, epochs=50, validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825a38f-08a4-4872-a0f5-a430f2a7a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e8208-4934-4d0b-9565-0c2acea5e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy and loss graphs\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
